# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched
summary(layoffs$Percentage)
min(layoffs$Percentage)
min(layoffs$Percentage, na.rm=TRUE)
summary(layoffs$Laid_Off_Count)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
summary(matched$Laid_Off_Count)
summary(matched$Percentage)
summary(matched[Percentage > 0.01]$Laid_Off_Count)
summary(matched$Laid_Off_Count)
matched
matched <- matched[Country == "United States"]
nrow(matched)
summary(matched$Laid_Off_Count)
summary(matched$Percentage)
matched[is.na(Percentage)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | is.na(Percentage) | is.na(Laid_Off_Count)]
matched
nrow(matched)
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
matched
head(matched)
nrow(matched)
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple matches
test <- matched[, .(Percentage = list(Percentage), Laid_Off_Count = list(Laid_Off_Count)), by = V2]
test
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ .id, value.var = "Date")
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ , value.var = "Date")
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ ., value.var = "Date")
test
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ Date, value.var = "Date")
test
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
# first create a count of the number of dates that starts at the lowest date
# sort by date
matched <- matched[, date := as.Date(Date, format = "%m/%d/%Y")]
matched
matched
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
matched$Date
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
# first create a count of the number of dates that starts at the lowest date
# sort by date
matched <- matched[, date := as.Date(Date, format = "%Y-%m-%d")]
matche
matched
matched$date
matched <- setkey(matched, COMPANY_NAME, date)
matched <- setkey(matched, V2, date)
matched
head(matched)
matched <- matched[, count := 1:.N, by = COMPANY_NAME]
matched <- matched[, count := 1:.N, by = V2]
head(matched)
# now reshape
matched <- dcast(matched, V2 + V1 ~ count, value.var = "date")
matched
?dcast
colnames(matched)[!colnames(matched) %in% c("V1", "V2")] <-
paste0("date_", colnames(matched)[!colnames(matched) %in% c("V1", "V2")])
matched
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
matched[, .(date := list(Date), by=V2]
matched[, .(date := list(Date)), by=V2]
matched[, .(date = list(Date)), by=V2]
matched[V2 == "google"]
matched <- matched[, .(date = list(Date)), by=V2]
matched[V2 == "google"]
matched[V2 == "google"]$date
setwd("/Users/davidvandijcke/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCos")
devtools::load_all()
### Load data ####
conf <- list()
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$`spark.executor.memory` <- "2GB"
conf$`spark.driver.maxResultSize` <- "2GB"
sc <- spark_connect(master = "local", version=3.5, config=conf)
## load data and filter in spark before passing to data table
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
sparkdf <- sparkdf %>% select(-c("NAICS"))
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf <- sparkdf %>% select(id_col, month, time_col, tenure, edu_days, title_categorical, COMPANY_NAME, RTO_date_manual)
sparkdf
sparkdf %>% filter(is.na(RTO_date_manual))
temp <- sparkdf %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% summarise(nobs = min(nobs)) %>%
collect()
temp
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% summarise(nobs = min(nobs)) %>%
collect()
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% summarise(nobs = min(nobs)) %>%
collect()
temp
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp
temp
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs))
temp
test <- temp %>% summarise(nobs = min(nobs)) %>%
collect()
test
temp %>% filter(nobs == 2)
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% collect()
temp
summary(temp$nobs)
?order
setorder(temp, nobs)
temp
temp <- sparkdf %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% collect()
temp
temp[nobs > 1000]
temp[temp$nobs > 1000,]
temp <- sparkdf %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% collect() %>% setDT()
temp <- temp[nobs > 1000]
tmep
temp
summary(temp)
head(temp)
fwrite(temp, file.path(dataOut, 'names_to_match.csv'))
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
# layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
tech <- fread(file.path(dataOut, 'names_to_match.csv'))
tech
test <- bind_rows(lapply(unique(tolower(tech$COMPANY_NAME)),
function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
)
temp <- test %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
temp <- temp %>% setDT()
# only retain the minimum cosine, jw, or lv
temp <- temp[, min_cosine := min(cosine), by = V1]
temp <- temp[, min_jw := min(jw), by = V1]
temp <- temp[, min_lv := min(lv), by = V1]
temp <- temp[cosine == min_cosine | jw == min_jw | lv == min_lv,]
fwrite(temp, file.path(dataOut, 'layoffs_matching.csv'))
sparkdf
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
sparkdf <- sparkdf %>% select(-c("NAICS"))
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf
temp
uniqueN(temp$V1)
uniqueN(temp$V2)
uniqueN(tech$COMPANY_NAME)
## get the manually corrected file and construct a wide version
matched <- fread(file.path(dataOut, 'layoffs_matching_manual_v2.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
test <- dcast(matched, V2 ~ Date)
test
# matched <- matched[, .(date = list(Date)), by=V2]
test <- setkey(matched, V2, Date)
test
head(test)
test[, count := 1:.N, by = V2]
head(test)
test <- dcast(matched, V2 ~ count, value.var="Date")
test <- dcast(test, V2 ~ count, value.var="Date")
test <- setkey(matched, V2, Date)
test[, count := 1:.N, by = V2]
head(test)
woop <- dcast(test, V2 ~ count, value.var="Date")
woop
fwrite(matched, file.path(dataOut, 'layoffs_matched_manual_v2.csv'))
ppl <-fread(file.path(dataIn, "people_tenure_prepped.csv.gz"))
ppl
ppl[COMPANY_NAME == "apple"]
#### Prepare paths and package ####
packages_load <- c("data.table", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr", "devtools", "DiSCos")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
# setwd("/home/dvdijcke/")
# system("rm -r DiSCos")
# system("git clone https://github.com/Davidvandijcke/DiSCos")
# setwd("DiSCos") # remove evmix from dependencies cause cant install on fucking server
# devtools::install(dependencies=FALSE)
codeDir <-   here::here() # "/home/dvdijcke/ppl/code/"
setwd(codeDir) # sets cd to program directory
dir <- sub("/[^/]+$", "", codeDir)# get main directory  "/home/dvdijcke/ppl"
dataIn <- file.path(dir, "data", "in")
dataOut <- file.path(dir, "data", "out")
tabs <- file.path(dir, "results", "tabs")
figs <- file.path(dir, "results", "figs") # "/Users/davidvandijcke/Dropbox (University of Michigan)/Apps/Overleaf/RTO/figs"
dataOut
ppl <- fread(file.path(dataOut, 'disco_prepped_microsoft.csv.gz'))
ppl <- as.data.table(ppl)
ppl <- ppl[, date := as.Date(RTO_date_manual, format = "%b %d %Y")]
ppl <- ppl[date >= endDate | is.na(date) | COMPANY_NAME == rtocomp]
# setwd("/Users/davidvandijcke/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCos")
# devtools::load_all()
### Load data ####
conf <- list()
Sys.setenv("SPARK_MEM" = "70g")
conf$`sparklyr.shell.driver-memory` <- "50G"
conf$`spark.executor.memory` <- "10GB"
conf$`spark.driver.maxResultSize` <- "10GB"
spark_install(version=3.5)
sc <- spark_connect(master = "local", version=3.5, config=conf)
## load data and filter in spark before passing to data table
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf <- sparkdf %>% mutate(naics_code = ifelse(COMPANY_NAME == "microsoft", 54151, naics_code))
# sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "apple", "Apr 11 2022", RTO_date_manual))
sparkdf <- sparkdf %>% mutate(naics_2 = substr(naics_code, 1, 2))
sparkdf <- sparkdf %>% filter(naics_2 %in% c("51", "54", "33"))
## filter out layoffs
layoffs <- fread(file.path(dataOut, 'layoffs_matched_manual_v2.csv'))
layoffs <- layoffs %>% select(V1, Date)
rtocomp <- "microsoft"
rtodate <- sparkdf %>% filter(COMPANY_NAME == rtocomp) %>% select(RTO_date_manual) %>%
distinct() %>% collect()
layoffcomp <- layoffs[V1 == rtocomp]
rtodate <- as.Date(rtodate$RTO_date_manual[1], format="%b %d %Y")
# assign startDate as 6 months before rtodate and endDate as 6 months after
firstDateOfMonth <- floor_date(rtodate, "month")
??floor_date
#### Prepare paths and package ####
packages_load <- c("data.table", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr", "devtools", "DiSCos", "floor_date")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
#### Prepare paths and package ####
packages_load <- c("data.table", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr", "devtools", "DiSCos", "lubridate")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
# setwd("/Users/davidvandijcke/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCos")
# devtools::load_all()
### Load data ####
conf <- list()
Sys.setenv("SPARK_MEM" = "70g")
conf$`sparklyr.shell.driver-memory` <- "50G"
conf$`spark.executor.memory` <- "10GB"
conf$`spark.driver.maxResultSize` <- "10GB"
spark_install(version=3.5)
sc <- spark_connect(master = "local", version=3.5, config=conf)
## load data and filter in spark before passing to data table
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf <- sparkdf %>% mutate(naics_code = ifelse(COMPANY_NAME == "microsoft", 54151, naics_code))
# sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "apple", "Apr 11 2022", RTO_date_manual))
sparkdf <- sparkdf %>% mutate(naics_2 = substr(naics_code, 1, 2))
sparkdf <- sparkdf %>% filter(naics_2 %in% c("51", "54", "33"))
## filter out layoffs
layoffs <- fread(file.path(dataOut, 'layoffs_matched_manual_v2.csv'))
layoffs <- layoffs %>% select(V1, Date)
rtocomp <- "microsoft"
rtodate <- sparkdf %>% filter(COMPANY_NAME == rtocomp) %>% select(RTO_date_manual) %>%
distinct() %>% collect()
layoffcomp <- layoffs[V1 == rtocomp]
rtodate <- as.Date(rtodate$RTO_date_manual[1], format="%b %d %Y")
# assign startDate as 6 months before rtodate and endDate as 6 months after
firstDateOfMonth <- floor_date(rtodate, "month")
startDate <- firstDateOfMonth %m-% months(8)
endDate <- firstDateOfMonth %m+% months(6)
ppl <- as.data.table(ppl)
ppl <- ppl[, date := as.Date(RTO_date_manual, format = "%b %d %Y")]
ppl <- ppl[date >= endDate | is.na(date) | COMPANY_NAME == rtocomp]
ppl[, y_col := tenure]
#ppl[, y_col := edu_days]
ppl <- ppl[, nobs_time := .N, by = c("id_col", "time_col")]
ppl <- ppl[, nobs := min(nobs_time), by = "id_col"]
test <- ppl
#
# # exclude companies with a larger than 1% drop in no of employees month to month
# agg <- test[, .(nobs_time = .N), by = c("id_col", "time_col")]
# agg <- setkey(agg, id_col, time_col)
# agg[, change := nobs_time - shift(nobs_time, 1), by = id_col]
# agg[, change_perc := change / shift(nobs_time, 1), by = id_col]
# agg[, change_max := min(change_perc, na.rm=TRUE), by = id_col]
# ids_keep <- unique(agg[change_max > -0.01]$id_col)
# test <- test[id_col %in% ids_keep | id_col]
# test <- test[time_col >= 2] # 6 months in advance
id_col.target <- unique(test[COMPANY_NAME == rtocomp]$id_col)
t0 <- unique(test[month == firstDateOfMonth]$time_col)
#-----
# try grouping by 4 months before and after
grouped <- test
grouped <- grouped[time_col %between% c(t0-8,t0+3)]
grouped[,time_col := findInterval(time_col, c(t0-8, t0-4, t0, t0+3))]
grouped <- grouped %>% group_by(time_col, ID, id_col) %>% summarize(y_col = max(y_col))
t0 <- 3
M <- 1000 #  max(list(uniqueN(ppl$id_col) * 1.5, 1000))
#-----
?DiSCo
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50, M=M, num.cores = 10,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod="qkden")
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod="qkden")
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
t0
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod=NULL)
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
rtocomp
t0 <- unique(test[month == firstDateOfMonth]$time_col)
t0
id_col.target <- unique(test[COMPANY_NAME == rtocomp]$id_col)
install.packages("DiSCos")
install.packages("DiSCos")
install.packages("DiSCos")
install.packages("DiSCos")
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod=NULL)
#### Prepare paths and package ####
packages_load <- c("data.table", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr", "devtools", "DiSCos", "lubridate")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
# setwd("/home/dvdijcke/")
# system("rm -r DiSCos")
# system("git clone https://github.com/Davidvandijcke/DiSCos")
# setwd("DiSCos") # remove evmix from dependencies cause cant install on fucking server
# devtools::install(dependencies=FALSE)
codeDir <-   here::here() # "/home/dvdijcke/ppl/code/"
setwd(codeDir) # sets cd to program directory
dir <- sub("/[^/]+$", "", codeDir)# get main directory  "/home/dvdijcke/ppl"
dataIn <- file.path(dir, "data", "in")
dataOut <- file.path(dir, "data", "out")
tabs <- file.path(dir, "results", "tabs")
figs <- file.path(dir, "results", "figs") # "/Users/davidvandijcke/Dropbox (University of Michigan)/Apps/Overleaf/RTO/figs"
# setwd("/Users/davidvandijcke/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCos")
# devtools::load_all()
### Load data ####
conf <- list()
Sys.setenv("SPARK_MEM" = "70g")
conf$`sparklyr.shell.driver-memory` <- "50G"
conf$`spark.executor.memory` <- "10GB"
conf$`spark.driver.maxResultSize` <- "10GB"
spark_install(version=3.5)
sc <- spark_connect(master = "local", version=3.5, config=conf)
## load data and filter in spark before passing to data table
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf <- sparkdf %>% mutate(naics_code = ifelse(COMPANY_NAME == "microsoft", 54151, naics_code))
# sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "apple", "Apr 11 2022", RTO_date_manual))
sparkdf <- sparkdf %>% mutate(naics_2 = substr(naics_code, 1, 2))
sparkdf <- sparkdf %>% filter(naics_2 %in% c("51", "54", "33"))
## filter out layoffs
layoffs <- fread(file.path(dataOut, 'layoffs_matched_manual_v2.csv'))
layoffs <- layoffs %>% select(V1, Date)
rtocomp <- "microsoft"
rtodate <- sparkdf %>% filter(COMPANY_NAME == rtocomp) %>% select(RTO_date_manual) %>%
distinct() %>% collect()
layoffcomp <- layoffs[V1 == rtocomp]
rtodate <- as.Date(rtodate$RTO_date_manual[1], format="%b %d %Y")
# assign startDate as 6 months before rtodate and endDate as 6 months after
firstDateOfMonth <- floor_date(rtodate, "month")
startDate <- firstDateOfMonth %m-% months(8)
endDate <- firstDateOfMonth %m+% months(6)
ppl <- fread(file.path(dataOut, 'disco_prepped_microsoft.csv.gz'))
ppl <- as.data.table(ppl)
ppl <- ppl[, date := as.Date(RTO_date_manual, format = "%b %d %Y")]
ppl <- ppl[date >= endDate | is.na(date) | COMPANY_NAME == rtocomp]
ppl[, y_col := tenure]
#ppl[, y_col := edu_days]
ppl <- ppl[, nobs_time := .N, by = c("id_col", "time_col")]
ppl <- ppl[, nobs := min(nobs_time), by = "id_col"]
test <- ppl
#
# # exclude companies with a larger than 1% drop in no of employees month to month
# agg <- test[, .(nobs_time = .N), by = c("id_col", "time_col")]
# agg <- setkey(agg, id_col, time_col)
# agg[, change := nobs_time - shift(nobs_time, 1), by = id_col]
# agg[, change_perc := change / shift(nobs_time, 1), by = id_col]
# agg[, change_max := min(change_perc, na.rm=TRUE), by = id_col]
# ids_keep <- unique(agg[change_max > -0.01]$id_col)
# test <- test[id_col %in% ids_keep | id_col]
# test <- test[time_col >= 2] # 6 months in advance
id_col.target <- unique(test[COMPANY_NAME == rtocomp]$id_col)
t0 <- unique(test[month == firstDateOfMonth]$time_col)
#-----
# try grouping by 4 months before and after
grouped <- test
grouped <- grouped[time_col %between% c(t0-8,t0+3)]
grouped[,time_col := findInterval(time_col, c(t0-8, t0-4, t0, t0+3))]
grouped <- grouped %>% group_by(time_col, ID, id_col) %>% summarize(y_col = max(y_col))
t0 <- 3
M <- 1000 #  max(list(uniqueN(ppl$id_col) * 1.5, 1000))
#-----
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod=NULL)
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod="qkden")
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 500,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod="qkden")
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 50,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod=NULL)
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
?data.table
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 100,
M=M, num.cores = 4,
permutation = FALSE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod=NULL)
summary(disco$perm)
discot <-  DiSCoTEA(disco,agg="quantileDiff", graph=TRUE)
disco <- DiSCos::DiSCo(grouped, id_col.target = id_col.target, t0 = t0, q_max=0.9, G = 100,
M=M, num.cores = 4,
permutation = TRUE, CI = FALSE, boots = 500, simplex=TRUE, seed=5,
qmethod=NULL)
summary(disco$perm)
install.packages("survival")
?clogit
library(survival)
?clogit
logan2
survival::logan
resp <- levels(logan$occupation)
n <- nrow(logan)
indx <- rep(1:n, length(resp))
logan2 <- data.frame(logan[indx,],
id = indx,
tocc = factor(rep(resp, each=n)))
logan2
head(logan2)
logan2$case <- (logan2$occupation == logan2$tocc)
head(logan2)
logan2[logan2$id == 1,]
