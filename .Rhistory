disco <- DiSCo(test, id_col.target = 3, t0 = 8, G = 1000, M=M, num.cores = 7,
permutation = TRUE, CI = TRUE, boots = 1000, simplex=FALSE, seed=5)
summary(disco$perm)
discot <-  DiSCoTEA(disco,  agg="quantileDiff", graph=TRUE, ylim=c(-150, 50))
#### Prepare paths and package ####
packages_load <- c("data.table", "DiSCos", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
codeDir <- here::here()
setwd(codeDir) # sets cd to program directory
dir <- sub("/[^/]+$", "", codeDir)# get main directory
dataIn <- file.path(dir, "data", "in")
dataOut <- file.path(dir, "data", "out")
tabs <- file.path(dir, "results", "tabs")
figs <- "/Users/davidvandijcke/Dropbox (University of Michigan)/Apps/Overleaf/RTO/figs"
setwd("/Users/davidvandijcke/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCos")
devtools::load_all()
### Load data ####
conf <- list()
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$`spark.executor.memory` <- "2GB"
conf$`spark.driver.maxResultSize` <- "2GB"
sc <- spark_connect(master = "local", version=3.5, config=conf)
## load data and filter in spark before passing to data table
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
sc <- spark_connect(master = "local", version=3.5, config=conf)
#### Prepare paths and package ####
packages_load <- c("data.table", "DiSCos", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
codeDir <- here::here()
setwd(codeDir) # sets cd to program directory
dir <- sub("/[^/]+$", "", codeDir)# get main directory
dataIn <- file.path(dir, "data", "in")
dataOut <- file.path(dir, "data", "out")
tabs <- file.path(dir, "results", "tabs")
figs <- "/Users/davidvandijcke/Dropbox (University of Michigan)/Apps/Overleaf/RTO/figs"
#### Prepare paths and package ####
packages_load <- c("data.table", "DiSCos", "here", "ggtext", "changepoint", "tidyverse",
"sparklyr")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
codeDir <- here::here()
setwd(codeDir) # sets cd to program directory
dir <- sub("/[^/]+$", "", codeDir)# get main directory
dataIn <- file.path(dir, "data", "in")
dataOut <- file.path(dir, "data", "out")
tabs <- file.path(dir, "results", "tabs")
figs <- "/Users/davidvandijcke/Dropbox (University of Michigan)/Apps/Overleaf/RTO/figs"
# calculate RTO dates
rto <- fread(file.path(dataIn, 'tech_companies_rto,csv'))
# calculate RTO dates
rto <- fread(file.path(dataIn, 'tech_companies_rto/csv'))
# calculate RTO dates
rto <- fread(file.path(dataIn, 'tech_companies_rto.csv'))
rto
head(rto)
rto <- rto[RTO_date_manual != "missing"]
rto <- rto[RTO_1 != "missing"]
summary(rto$RTO_1)
is.na(rto$RTO_1)
rto
# calculate RTO dates
rto <- fread(file.path(dataIn, 'tech_companies_rto.csv'))
rto
is.na(rto$RTO_1)
rto$RTO_1 == "missing"
rto <- rto[(RTO_1 != "missing") | is.na(RTO_1)]
rto
summary(rto)
mean(is.na(rto$RTO_1))
sum(is.na(rto$RTO_1))
sum(!is.na(rto$RTO_1))
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
layoffs
tech <- fread(file.path(dataIn, 'tech_companies_rto.csv'))
tech
install.packages("tidystringdist")
library(tidystringdist)
test <- tidystringdist::tidy_comb_all(layoff_names$COMPANY_NAME, unique(tech$`Company Name`)) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
tech <- fread(file.path(dataIn, 'tech_companies_rto.csv'))
test <- tidystringdist::tidy_comb_all(layoffs$COMPANY_NAME, unique(tech$`Company Name`)) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test
uniqueN(test$V2)
uniqueN(test$V1)
?tidy_comb_all
Species
iris
test <- tidystringdist::tidy_comb_all(layoffs$COMPANY_NAME, unique(tech$`Company Name`))
test <- tidystringdist::tidy_comb(layoffs$COMPANY_NAME, unique(tech$`Company Name`))
test
test <- tidystringdist::tidy_comb(layoffs$COMPANY_NAME, unique(tolower(tech$`Company Name`)))
uniqueN(test$V1)
uniqueN(test$V2)
test <- tidystringdist::tidy_comb(layoffs$COMPANY_NAME, unique(tolower(tech$`Company Name`))) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test
uniqueN(test$V2)
uniqueN(test$V1)
test <- test %>% setDT()
test[, min_cosine := min(cosine), by = V1]
test <- test[cosine == min_cosine]
test
test <- tidystringdist::tidy_comb(layoffs$COMPANY_NAME, unique(tolower(tech$`Company Name`))) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test <- test %>% setDT()
test
View(test)
test[cosine < 0.1]
test[cosine < 0.2]
test[jw < 0.2]
test[jw < 0.3]
test <- tidystringdist::tidy_comb(layoffs$COMPANY_NAME, unique(tolower(tech$`Company Name`)))
test[v1 == "microsoft"]
test[V1 == "microsoft"]
test[test$V1 == "microsoft"]
test <- test %>% setDT()
test[V1 == "microsoft"]
test[V2 == "microsoft"]
test <- tidystringdist::tidy_comb(layoffs$COMPANY_NAME, unique(tolower(tech$`Company Name`))) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test <- test %>% setDT()
test[jw < 01]
test[jw < 0.1]
test[jw < 0.5]
test[jw < 0.1]
test[jw < 0.2]
test[jw < 0.3]
?tidy_comb
test <- tidystringdist::tidy_comb(unique(tolower(tech$`Company Name`), layoffs$COMPANY_NAME)) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test <- tidystringdist::tidy_comb(unique(tolower(tech$`Company Name`), layoffs$COMPANY_NAME)) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test <- tidystringdist::tidy_comb(unique(tolower(tech$`Company Name`)), layoffs$COMPANY_NAME) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test
test <- test %>% setDT()
test[jw < 0.1]
test[jw < 0.2]
test[V1 == "microsoft"]
test[V1 == "microsoft" & V2 == "microsoft"]
test <- tidystringdist::tidy_comb_all(unique(tolower(tech$`Company Name`)), layoffs$COMPANY_NAME) %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
test
uniqueN(test$V1)
uniqueN(test$V)
uniqueN(test$V2)
test <- lapply(unique(tolower(tech$`Company Name`)), function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
test <- data.frame(test)
test
test <- sapply(unique(tolower(tech$`Company Name`)), function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
test
data.frame(test)
test <- lapply(unique(tolower(tech$`Company Name`)), function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
test
bind_rows(test)
test <- bind_rows(lapply(unique(tolower(tech$`Company Name`)),
function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
)
test
temp <- test %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
temp
uniqueN(temp$V1)
uniqueN(temp$V2)
temp <- temp %>% setDT()
temp[, min_cosine := min(cosine), by = V1]
temp <- temp[cosine == min_cosine]
temp
uniquen(temp$V1)
uniqueN(temp$V1)
test[V2 == "zoom"]
test[test$V2 == "zoom",]
temp <- test %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
temp <- temp %>% setDT()
# only retain the minimum cosine, jw, or lv
temp <- temp[, min_cosine := min(cosine), by = V1]
temp <- temp[, min_jw := min(jw), by = V1]
temp <- temp[, min_lv := min(lv), by = V1]
temp <- temp[, cosine == min_cosine | jw == min_jw | lv == min_lv]
temp
temp <- test %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
temp <- temp %>% setDT()
# only retain the minimum cosine, jw, or lv
temp <- temp[, min_cosine := min(cosine), by = V1]
temp <- temp[, min_jw := min(jw), by = V1]
temp <- temp[, min_lv := min(lv), by = V1]
temp <- temp[cosine == min_cosine | jw == min_jw | lv == min_lv,]
temp
fwrite(temp, file.path(dataOut, 'layoffs_matching.csv')
)
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
# layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
tech <- fread(file.path(dataIn, 'tech_companies_rto.csv'))
test <- bind_rows(lapply(unique(tolower(tech$`Company Name`)),
function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
)
temp <- test %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
temp <- temp %>% setDT()
# only retain the minimum cosine, jw, or lv
temp <- temp[, min_cosine := min(cosine), by = V1]
temp <- temp[, min_jw := min(jw), by = V1]
temp <- temp[, min_lv := min(lv), by = V1]
temp <- temp[cosine == min_cosine | jw == min_jw | lv == min_lv,]
fwrite(temp, file.path(dataOut, 'layoffs_matching.csv'))
layoffs <- fread(file.path(dataIn, 'layoffs_matching_manual.csv'))
layoffs <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
layoffs
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
# layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
layoffs <- layoffs %>% left_join(matched, by = c("COMPANY_NAME" = "V2"))
layoffs
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
temp <- matched[layoffs, on = c("V1" = "COMPANY_NAME"), how = "inner"]
temp <- matched[layoffs, on = c("V1" = "COMPANY_NAME")]
matched
layoffs
# layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
temp <- matched[layoffs, on = c("V1" = "COMPANY_NAME")]
matched
matched <- matched[!duplicated(matched)]
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME")]
matched
?merge
# inner join with layoffs
layoffs <- merge(layoffs, matched, by.x = "COMPANY_NAME", by.y = "V2")
layoffs
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
# layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V1" = "COMPANY_NAME")]
matched
nrow(matched)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V1" = "COMPANY_NAME"), nomatch = NULL]
matched
head9
head(matched)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
uniqueN(matched$V2)
uniqueN(matched$V1)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched
nrow(matched)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
nrow(matched)
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
nrow(matched)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched
summary(layoffs$Percentage)
min(layoffs$Percentage)
min(layoffs$Percentage, na.rm=TRUE)
summary(layoffs$Laid_Off_Count)
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
summary(matched$Laid_Off_Count)
summary(matched$Percentage)
summary(matched[Percentage > 0.01]$Laid_Off_Count)
summary(matched$Laid_Off_Count)
matched
matched <- matched[Country == "United States"]
nrow(matched)
summary(matched$Laid_Off_Count)
summary(matched$Percentage)
matched[is.na(Percentage)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | is.na(Percentage) | is.na(Laid_Off_Count)]
matched
nrow(matched)
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
matched
head(matched)
nrow(matched)
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple matches
test <- matched[, .(Percentage = list(Percentage), Laid_Off_Count = list(Laid_Off_Count)), by = V2]
test
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ .id, value.var = "Date")
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ , value.var = "Date")
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ ., value.var = "Date")
test
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
test <- dcast(matched, V2 + V1 ~ Date, value.var = "Date")
test
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
# first create a count of the number of dates that starts at the lowest date
# sort by date
matched <- matched[, date := as.Date(Date, format = "%m/%d/%Y")]
matched
matched
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
matched$Date
# now keep only one row per COMPANY_NAME but create multiple columns if it has multiple Dates
# first create a count of the number of dates that starts at the lowest date
# sort by date
matched <- matched[, date := as.Date(Date, format = "%Y-%m-%d")]
matche
matched
matched$date
matched <- setkey(matched, COMPANY_NAME, date)
matched <- setkey(matched, V2, date)
matched
head(matched)
matched <- matched[, count := 1:.N, by = COMPANY_NAME]
matched <- matched[, count := 1:.N, by = V2]
head(matched)
# now reshape
matched <- dcast(matched, V2 + V1 ~ count, value.var = "date")
matched
?dcast
colnames(matched)[!colnames(matched) %in% c("V1", "V2")] <-
paste0("date_", colnames(matched)[!colnames(matched) %in% c("V1", "V2")])
matched
matched <- fread(file.path(dataOut, 'layoffs_matching_manual.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
matched[, .(date := list(Date), by=V2]
matched[, .(date := list(Date)), by=V2]
matched[, .(date = list(Date)), by=V2]
matched[V2 == "google"]
matched <- matched[, .(date = list(Date)), by=V2]
matched[V2 == "google"]
matched[V2 == "google"]$date
setwd("/Users/davidvandijcke/Dropbox (University of Michigan)/Flo_GSRA/repo/DiSCos")
devtools::load_all()
### Load data ####
conf <- list()
conf$`sparklyr.shell.driver-memory` <- "16G"
conf$`spark.executor.memory` <- "2GB"
conf$`spark.driver.maxResultSize` <- "2GB"
sc <- spark_connect(master = "local", version=3.5, config=conf)
## load data and filter in spark before passing to data table
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
sparkdf <- sparkdf %>% select(-c("NAICS"))
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf <- sparkdf %>% select(id_col, month, time_col, tenure, edu_days, title_categorical, COMPANY_NAME, RTO_date_manual)
sparkdf
sparkdf %>% filter(is.na(RTO_date_manual))
temp <- sparkdf %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% summarise(nobs = min(nobs)) %>%
collect()
temp
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% summarise(nobs = min(nobs)) %>%
collect()
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% summarise(nobs = min(nobs)) %>%
collect()
temp
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp
temp
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs))
temp
test <- temp %>% summarise(nobs = min(nobs)) %>%
collect()
test
temp %>% filter(nobs == 2)
# now filter based on the smallest treated company
temp <- sparkdf %>% filter(!is.na(RTO_date_manual))
temp <- temp %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% collect()
temp
summary(temp$nobs)
?order
setorder(temp, nobs)
temp
temp <- sparkdf %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% collect()
temp
temp[nobs > 1000]
temp[temp$nobs > 1000,]
temp <- sparkdf %>%
group_by(time_col, COMPANY_NAME) %>% summarise(nobs = n()) %>%
group_by(COMPANY_NAME) %>% summarise(nobs = min(nobs)) %>% collect() %>% setDT()
temp <- temp[nobs > 1000]
tmep
temp
summary(temp)
head(temp)
fwrite(temp, file.path(dataOut, 'names_to_match.csv'))
# merge in tech layoffs data
layoffs <- fread(file.path(dataIn, 'layoffs_data(1).csv'))
# layoffs <- layoffs[Country == "United States"]
layoffs[, COMPANY_NAME := tolower(Company)]
tech <- fread(file.path(dataOut, 'names_to_match.csv'))
tech
test <- bind_rows(lapply(unique(tolower(tech$COMPANY_NAME)),
function(x) tidystringdist::tidy_comb(layoffs$COMPANY_NAME, x))
)
temp <- test %>%
tidy_stringdist(method = c("jw", "lv", "cosine"))
temp <- temp %>% setDT()
# only retain the minimum cosine, jw, or lv
temp <- temp[, min_cosine := min(cosine), by = V1]
temp <- temp[, min_jw := min(jw), by = V1]
temp <- temp[, min_lv := min(lv), by = V1]
temp <- temp[cosine == min_cosine | jw == min_jw | lv == min_lv,]
fwrite(temp, file.path(dataOut, 'layoffs_matching.csv'))
sparkdf
sparkdf <- spark_read_parquet(sc, name = "people_tenure_prepped",
path = file.path(dataIn, "people_tenure_prepped/*"),
memory = FALSE)
sparkdf <- sparkdf %>% select(-c("NAICS"))
# change RTO_date_manual to May 1 2023 for COMPANY_NAME amazon
sparkdf <- sparkdf %>% mutate(RTO_date_manual = ifelse(COMPANY_NAME == "amazon", "May 1 2023", RTO_date_manual))
sparkdf
temp
uniqueN(temp$V1)
uniqueN(temp$V2)
uniqueN(tech$COMPANY_NAME)
## get the manually corrected file and construct a wide version
matched <- fread(file.path(dataOut, 'layoffs_matching_manual_v2.csv'))
matched <- matched[!duplicated(matched)]
# inner join with layoffs
matched <- matched[layoffs, on = c("V2" = "COMPANY_NAME"), nomatch = NULL]
matched <- matched[Country == "United States"]
matched[, both_na := is.na(Percentage) & is.na(Laid_Off_Count)]
matched <- matched[(Percentage > 0.01 | Laid_Off_Count > 100) | both_na]
test <- dcast(matched, V2 ~ Date)
test
# matched <- matched[, .(date = list(Date)), by=V2]
test <- setkey(matched, V2, Date)
test
head(test)
test[, count := 1:.N, by = V2]
head(test)
test <- dcast(matched, V2 ~ count, value.var="Date")
test <- dcast(test, V2 ~ count, value.var="Date")
test <- setkey(matched, V2, Date)
test[, count := 1:.N, by = V2]
head(test)
woop <- dcast(test, V2 ~ count, value.var="Date")
woop
fwrite(matched, file.path(dataOut, 'layoffs_matched_manual_v2.csv'))
ppl <-fread(file.path(dataIn, "people_tenure_prepped.csv.gz"))
ppl
ppl[COMPANY_NAME == "apple"]
